{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert dicts\n",
    "def invert_dict_singles(orig):\n",
    "    new = {}\n",
    "    for k, v in orig.items():\n",
    "        new[v] = new.get(v, [])\n",
    "        new[v].append(k)\n",
    "    return new\n",
    "\n",
    "def invert_dict_lists(orig):\n",
    "    new = {}\n",
    "    for k, vals in orig.items():\n",
    "        for v in vals:\n",
    "            new[v] = k\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dupes(lst):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in lst if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "lang_trans = {}\n",
    "with open('data/lang_tags.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = DictReader(file, fieldnames=['word', 'lang'])\n",
    "    for row in reader:\n",
    "        lang_trans[row['word']] = row['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack instead of bigger solution that didn't work\n",
    "spellcheck = {\n",
    "    'reduc': 'reduced',\n",
    "    'jell o': 'gelatin',\n",
    "    'jello': 'gelatin',\n",
    "    'made with': 'with',\n",
    "    'v': 'tomato juice',\n",
    "    'e fu': 'yi mein',\n",
    "    'miracle whip': 'mayonaise',\n",
    "    'ragu': 'italian',\n",
    "    'veget': 'vegetable',\n",
    "    'vegeta': 'vegetable',\n",
    "    'recip': 'recipe',\n",
    "    'oliv': 'olive',\n",
    "    'semi sweet': 'semisweet',\n",
    "    'mellow yellow': 'soda',\n",
    "    'mountain dew': 'soda',\n",
    "    'budweiser': 'beer',\n",
    "    'soi': 'soy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['accompaniment', 'all', 'boil', 'bought', 'boston', 'boned', 'boneless', 'breast', 'breasts', 'broiler',\n",
    "         'can', 'canned', 'chopped', 'crumbled', 'crumbles', 'crushed', 'cuisine', 'cuisines', \n",
    "         'cut', 'center', 'coarse', 'cook', 'cooking', 'dried', 'dry', 'diced', 'extra', \n",
    "         'fast', 'filled', 'fine', 'finely', 'fresh', 'freshly', 'frozen', 'fryer', 'full', 'grate', 'grated', 'grill', 'ground', \n",
    "         'half', 'halves', 'head', 'high', 'homestyl', 'homestyle', 'large', 'leaf', 'leftover', 'leg', 'legs', 'low',\n",
    "         'master', 'medium', 'minced', 'natural',\n",
    "         'original', 'ounc', 'oven', 'oz', 'part', 'plain', 'premium', 'purpose', 'ready',\n",
    "         'real', 'reduce', 'refrigerated', 'rising', 'skinned', 'skinless', 'sodium',\n",
    "         'seamless', 'sheet', 'shred', 'shredded', 'slice', 'sliced', 'small', 'standing', 'streaky', 'store', 'style', 'superior', \n",
    "         'thigh', 'thighs', 'torn', 'traditional', 'unsweetened', 'virgin', 'wedges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sub_classes = {\n",
    "    '': stopwords,\n",
    "    'american': ['argo', 'bisquick', 'bragg', 'braggs', 'breakstone', 'breakstones', 'breyers', 'crisco',\n",
    "              'campbells', 'diamond', 'frenchs', 'heinz', 'hellmanns', 'hellmann', 'hershey',\n",
    "              'hurst', 'jif', 'jiffy', 'johnsonville', 'jonshonville', 'kerrygold', 'klondike', 'knorr', 'knudsen', 'kraft', 'lipton',\n",
    "              'mazola', 'meyer', 'mccormick', 'nestle', 'pam', 'pace', 'philadelphia', 'pillsbury', 'pompeian', 'progresso', 'ritz', 'smithfield',\n",
    "              'sargento', 'stonefire', 'swanson', 'triscuits', 'tyson',\n",
    "              'wishbone', 'wondra', 'williams', 'yoplait', 'zatarains'],\n",
    "    'italian': ['barilla', 'bertolli', 'classico', 'delallo'],\n",
    "    'mexican': ['mission', 'goya', 'rotel', 'tostidos'],\n",
    "    'healthy': ['diet', 'fatfree', 'glutenfree', 'light', 'lowfat',  'lowsodium', 'nonfat']\n",
    "}\n",
    "\n",
    "phrase_sub_classes = {\n",
    "    '': ['firmly packed', 'flat leaf', 'free range', 'on the vine', 'rapid rise', 'vine ripened', 'whole kernel'],\n",
    "    'american': ['artisan blends', 'best food', 'better than bouillon', 'betty crocker', 'bob evans', 'calcium plus vitamin d', 'country crock',\n",
    "              'crystal farms', 'duncan hines', 'earth balance', 'egglands best', 'family harvest', 'farmhouse original', \n",
    "              'foster farms', 'franks redhot', 'gold medal', 'good seasons', 'gourmet garden',\n",
    "              'green giant', 'hidden valley', 'hillshire farms', 'home originals', 'honeysuckle white',\n",
    "              'i cant believe its not', 'i cant believe it not', 'i cant believ it not', 'jimmy dean', 'king arthur', 'land o lakes',\n",
    "              'lea and perrins', 'mrs dash', 'nielsen massey', 'no stick', 'oscar mayer', \n",
    "              'pasta sides', 'pepperidge farm', 'pure wesson', 'ready rice', 'recipe creation', 'recipe secret', 'robert mondavi',\n",
    "              'simply organic', 'skippy', 'special k', 'spice islands', 'a hint of', 'a touch of philadelphia', \n",
    "              'texas pete', 'uncle bens', 'wish bone', 'honey bunches of oats'],\n",
    "    'asian': ['a taste of thai', 'conimex woksaus specials', 'soy vay', 'veri veri'],\n",
    "    'italian': ['old world style'],\n",
    "    'mexican': ['old el paso', 'ro tel', 'taco bell', 'thick and chunky'],\n",
    "    'healthy': ['cholesterol free', 'fat free', 'gluten free', 'less sodium', 'low fat', 'low salt', 'low sodium', 'lower sodium', \n",
    "                'non fat', 'no salt added', 'reduced fat', 'reduce sodium', 'reduced sodium', 'sodium reduced', 'wheat free'],\n",
    "    # ands\n",
    "    'half-and-half': ['half and half'],\n",
    "    'mac-and-cheese': ['macaroni and cheese'],\n",
    "    'bread-and-butter': ['bread and butter'],\n",
    "    'chocolates': ['m and ms'],\n",
    "    'pork-and-beans': ['pork and beans'],\n",
    "    'sweet-and-sour': ['sweet and sour'],\n",
    "    # synonyms\n",
    "    'pepper': ['black pepper'],\n",
    "    'garlic': ['garlic clove', 'garlic cloves'],\n",
    "    'sugar': ['white sugar', 'granulated sugar'],\n",
    "    'cream': ['whipping cream', 'heavy cream', 'heavy whipping cream'],\n",
    "    'cumin': ['cumin seed', 'cumin seeds']\n",
    "}\n",
    "words_to_segment = {\n",
    " 'alfredostyle',\n",
    " 'almondmilk',\n",
    " 'applesauce',\n",
    " 'beetroot',\n",
    " 'bellpepper',\n",
    " 'blackcurrant',\n",
    " 'blackpepper',\n",
    " 'bluefish',\n",
    " 'boysenberries',\n",
    " 'breadcrumb',\n",
    " 'breadcrumbs',\n",
    " 'breadstick',\n",
    " 'cauliflowerets',\n",
    " 'chilegarlic',\n",
    " 'chocolatecovered',\n",
    " 'cornbread',\n",
    " 'corncobs',\n",
    " 'cornflour',\n",
    " 'cornstarch',\n",
    " 'crabapples',\n",
    " 'crawfish',\n",
    " 'crayfish',\n",
    " 'cuttlefish',\n",
    " 'ducklings',\n",
    " 'fruitcake',\n",
    " 'huckleberries',\n",
    " 'kingfish',\n",
    " 'kiwifruit',\n",
    " 'mexicorn',\n",
    " 'milkfish',\n",
    " 'monkfish',\n",
    " 'poppyseeds',\n",
    " 'poundcake',\n",
    " 'quickcooking',\n",
    " 'redcurrant',\n",
    " 'rockfish',\n",
    " 'sablefish',\n",
    " 'sheepshead',\n",
    " 'shellfish',\n",
    " 'sweetbreads',\n",
    " 'swordfish',\n",
    " 'wheatberries',\n",
    " 'whitefish',\n",
    " 'wolfberries'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sub = invert_dict_lists(word_sub_classes)\n",
    "phrases_to_sub = invert_dict_lists(phrase_sub_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "char_pattern = re.compile(r'[®™’â€/\\!\\'%\\(\\)\\.\\d]')\n",
    "of_pattern = re.compile(r' of (?:the )?')\n",
    "\n",
    "spellcheck_compiled = []\n",
    "for k, v in sorted(spellcheck.items()):\n",
    "    k = re.compile(r'(\\b)' + k + r'(\\b)')\n",
    "    v = r'\\1' + v + r'\\2'\n",
    "    spellcheck_compiled.append((k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def make_freq_dict(word_counts):\n",
    "    lines = []\n",
    "    for word, freq in sorted(word_counts.items(), key=itemgetter(1), reverse=True):\n",
    "        if freq > 4 and len(word) > 3:\n",
    "            lines.append('{0} {1}\\n'.format(word, freq))\n",
    "    with open('data/frequency_dictionary.txt', 'w+', encoding='utf-8') as file:\n",
    "        for line in lines:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(83000, 1, 7)\n",
    "dictionary_path = 'data/frequency_dictionary.txt'\n",
    "if not sym_spell.load_dictionary(dictionary_path, 0, 1):\n",
    "    print(\"Dictionary file not found\")\n",
    "\n",
    "def correct_spelling(word):\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.TOP, 1)\n",
    "    if not suggestions:\n",
    "        return word\n",
    "    return suggestions[0].term\n",
    "\n",
    "sym_seg = SymSpell(83000, 1, 8)\n",
    "dictionary_path = 'data/segment_dictionary.txt'\n",
    "if not sym_seg.load_dictionary(dictionary_path, 0, 1):\n",
    "    print(\"Dictionary file not found\")\n",
    "\n",
    "def segment_word(word):\n",
    "    res = sym_seg.word_segmentation(word)\n",
    "    return res.segmented_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "from functools import lru_cache\n",
    "import spacy\n",
    "\n",
    "parse = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "\n",
    "for word in parse.Defaults.stop_words:\n",
    "    lex = parse.vocab[word]\n",
    "    lex.is_stop = True\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def lemmatize(phrase):\n",
    "    return [token.lemma_ for token in parse(phrase) if not (token.is_stop or token.text in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counts = lambda df: df.apply(pd.Series.value_counts, axis=0)\n",
    "\n",
    "def get_errors(X, y, model, sort_col=None):\n",
    "    errors = []\n",
    "    for i in range(X.shape[0]):\n",
    "        obs = X.iloc[i:i+1]\n",
    "        real = y.iloc[i]\n",
    "        y_pred = model.predict(obs)\n",
    "        if y_pred != [real]:\n",
    "            errors.append(i)\n",
    "    errs = pd.concat([X.iloc[errors], y.iloc[errors]], axis=1)\n",
    "    print('Errors:', errs.shape[0])\n",
    "    if sort_col:\n",
    "        errs.sort_values(sort_col, inplace=True)\n",
    "    return errs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

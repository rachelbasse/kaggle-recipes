{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict, Counter\n",
    "from csv import DictReader\n",
    "#from googletrans import Translator\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "### Numpy Print Options ###\n",
    "np.set_printoptions(\n",
    "    threshold=2000, # 1000\n",
    "    edgeitems=10, # 3\n",
    "    linewidth=180, # 75\n",
    "    precision=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('data/train.json', orient='records', encoding='utf-8')\n",
    "test = pd.read_json('data/test.json', orient='records', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', drop=True, inplace=True)\n",
    "test.set_index('id', drop=True, inplace=True)\n",
    "test.insert(0, 'cuisine', 'test')\n",
    "data = pd.concat((train, test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack instead of bigger solution that didn't work\n",
    "spellcheck = {\n",
    "    r'i cant believe? its? not': '',\n",
    "    'sauc': 'sauce',\n",
    "    'recip': 'recipe',\n",
    "    'reduc': 'reduced',\n",
    "    'jonshonville': 'johnsonville',\n",
    "    'burgundi': 'burgundy',\n",
    "    'jell o': 'gelatin',\n",
    "    'jello': 'gelatin',\n",
    "    'made with': 'with',\n",
    "    'v': 'v_eight',\n",
    "    'e fu': 'yi mein',\n",
    "    'fatfree': 'fat free',\n",
    "    'miracle whip': 'mayonaise'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_sub_classes = {\n",
    "    # to remove\n",
    "    '': [' oz ', 'bone in', 'skin on', 'head on', 'on the vine', 'i cant believe its not', 'refrigerated', 'store bought'],\n",
    "    'brand': ['argo', 'artisan blends', 'best foods', 'big slice', 'bisquick', 'breakstone', 'campbells', 'country crock', \n",
    "              'crystal farms', 'duncan hines', 'egglands best', 'family harvest', 'farmhouse originals', \n",
    "              'foster farms', 'franks redhot', 'frenchs', 'good seasons', 'gourmet garden',\n",
    "              'green giant', 'heinz', 'hellmann', 'hershey', 'hidden valley', 'home originals', 'honeysuckle white',\n",
    "              'hurst', 'jimmy dean', 'johnsonville', 'king arthur', 'klondike', 'knorr', 'knudsen', 'kraft', 'land o lakes', 'lipton',\n",
    "              'lipton recipe secret', 'nestle', 'nielsen massey', 'no stick', 'oscar mayer', \n",
    "              'pam', 'pasta sides', 'pepperidge farm', 'pillsbury', \n",
    "              'pompeian', 'pure wesson', 'ready rice', 'recipe creations', 'recipe secret', 'recipe secrets',\n",
    "              'sargento', 'simply organic', 'spice islands', 'stonefire', 'a hint of', 'a touch of philadelphia', \n",
    "              'uncle bens', 'wish bone', 'wishbone', 'yoplait', 'lea and perrins', 'honey bunches of oats'],\n",
    "    'easian brand': ['a taste of thai', 'conimex woksaus specials', 'soy vay', 'veri veri'],\n",
    "    'italian brand': ['barilla', 'bertolli', 'delallo', 'old world style', 'ragu'],\n",
    "    'mexican brand': ['old el paso', 'rotel', 'ro tel', 'taco bell'],\n",
    "    'marketing': ['all natural', 'cholesterol free', 'classic', 'deli fresh', 'diet', 'family size', 'fat free',\n",
    "                  'gluten free', 'less sodium', 'low fat', 'low sodium', 'non fat', 'original', 'premium', \n",
    "                  'reduced fat', 'reduced sodium', 'thick and chunky', 'traditional'],\n",
    "    # ands\n",
    "    'half_and_half': ['half and half'],\n",
    "    'mac_and_cheese': ['macaroni and cheese'],\n",
    "    'bread_and_butter': ['bread and butter'],\n",
    "    'm_and_ms': ['m and ms'],\n",
    "    'pork_and_beans': ['pork and beans'],\n",
    "    'sweet_and_sour': ['sweet and sour'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_to_sub = invert_dict_lists(phrase_sub_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_trans = {}\n",
    "with open('data/rare_translations.csv', 'r', encoding='utf-8-sig') as file:\n",
    "    reader = DictReader(file, fieldnames=['k', 'v'])\n",
    "    for row in reader:\n",
    "        rare_trans[row['k']] = row['v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phrase(orig_phrase):\n",
    "    phrase = orig_phrase.lower()\n",
    "    \n",
    "    # remove useless chars\n",
    "    phrase = re.sub(r'[®™’â€/\\!\\'%\\(\\)\\.\\d]', '', phrase)\n",
    "    \n",
    "    # standardize 'n' and '&' to 'and'; '-' to ' '\n",
    "    phrase = re.sub(r' ?\\& ?', ' and ', phrase)\n",
    "    phrase = re.sub(r' n ', ' and ', phrase)\n",
    "    phrase = re.sub(r'-', ' ', phrase)\n",
    "    \n",
    "    # remove 'in x' and 'for x'\n",
    "    split = phrase.split(' in ')\n",
    "    phrase = split[0]\n",
    "    split = phrase.split(' for ')\n",
    "    phrase = split[0]\n",
    "    \n",
    "    # remove prep instructions\n",
    "    split = phrase.split(',')\n",
    "    phrase = split[0]\n",
    "    \n",
    "    # move 'with x' phrase to front\n",
    "    split = phrase.split(' with ')\n",
    "    if len(split) > 1:\n",
    "        phrase = ' '.join([split[1], split[0]])\n",
    "    \n",
    "    # hacky spelling correction\n",
    "    for k, v in spellcheck.items():\n",
    "        key = r'(\\b)' + k + r'(\\b)'\n",
    "        phrase = re.sub(key, r'\\1' + v + r'\\2', phrase)        \n",
    "    \n",
    "    # substitute phrases\n",
    "    # TODO optimize?\n",
    "    for k, v in phrases_to_sub.items():\n",
    "        key = r'(\\b)' + k + r'(\\b)'\n",
    "        phrase = re.sub(key, r'\\1' + v + r'\\2', phrase)\n",
    "    \n",
    "    # sub ' of (the)? ' with '_of_'\n",
    "    phrase = re.sub(r' of (?:the )?', '_of_', phrase)\n",
    "    \n",
    "    # remove remaining stopwords\n",
    "    phrase = re.sub(' and | or | up ', ' ', phrase)\n",
    "    \n",
    "    # remove final-'s' from all words\n",
    "    phrase = re.sub(r's(\\b)', r'\\1', phrase)\n",
    "    \n",
    "    # remove single letters\n",
    "    phrase = re.sub(r'\\b\\w\\b', ' ', phrase)\n",
    "    \n",
    "    # trim whitespacce\n",
    "    phrase = re.sub(r'\\s+', ' ', phrase.strip())\n",
    "    \n",
    "    # check for empty phrases\n",
    "    if len(phrase) == 0:\n",
    "        print(orig_phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(phrases):\n",
    "    words = set()\n",
    "    for phrase in phrases:\n",
    "        phrase = clean_phrase(phrase)\n",
    "        if not phrase:\n",
    "            continue\n",
    "        split = phrase.split()\n",
    "        for i, word in enumerate(split):\n",
    "            if len(word) > 4:\n",
    "                split[i] = correct_spelling(word)\n",
    "            if word in rare_trans:\n",
    "                split[i] = rare_trans[word]\n",
    "        split[-1] = split[-1] + '-h'\n",
    "        words.update(split)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ingredients = data.ingredients.map(encode_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "_ = data.ingredients.map(words.extend)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = set()\n",
    "rare = {}\n",
    "for word, freq in word_counts.items():\n",
    "    if freq < 5:\n",
    "        rare[word] = ''\n",
    "        continue\n",
    "    if freq > 24000:\n",
    "        common.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {k: i for i, k in enumerate(word_counts)}\n",
    "col_names = [k for k, _ in sorted(indices.items(), key=itemgetter(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros((data.shape[0], len(col_names)), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ri, ings in enumerate(data.ingredients):\n",
    "    zeros[ri, [indices[ing] for ing in ings]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_df = pd.DataFrame(zeros, columns=col_names, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data, zeros_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.query('cuisine != \"test\"')\n",
    "train = train.drop(columns=['ingredients'])\n",
    "train_cuisine = train.cuisine\n",
    "train_cuisine.to_csv('data/cuisine.csv', header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['cuisine'])\n",
    "train.to_csv('data/rare_trans_cleaned_train.csv', header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.query('cuisine == \"test\"')\n",
    "test = test.drop(columns=['cuisine', 'ingredients'])\n",
    "test.to_csv('data/rare_trans_cleaned_test.csv', header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

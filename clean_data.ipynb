{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "from functools import lru_cache\n",
    "#from helpers import *\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt\n",
    "data = pd.read_csv('data/data_cleaned.csv', header=0, index_col=0, encoding='utf-8', \n",
    "                   converters={'ingredients': lambda x: x[2:-2].split(\"', '\")})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('data/train.json', orient='records', encoding='utf-8')\n",
    "test = pd.read_json('data/test.json', orient='records', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', drop=True, inplace=True)\n",
    "test.set_index('id', drop=True, inplace=True)\n",
    "test.insert(0, 'cuisine', 'test')\n",
    "data = pd.concat((train, test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=4096)\n",
    "def clean_phrase(orig_phrase):\n",
    "    phrase = orig_phrase.lower()\n",
    "    \n",
    "    # remove useless chars\n",
    "    phrase = char_pattern.sub('', phrase)\n",
    "    \n",
    "    # standardize 'n' and '&' to 'and'; '-' to ' '\n",
    "    phrase = ' and '.join(phrase.split('&'))\n",
    "    phrase = ' and '.join(phrase.split(' n '))\n",
    "    phrase = ' '.join(phrase.split('-'))\n",
    "    \n",
    "    # remove prep instructions\n",
    "    split = phrase.split(',')\n",
    "    phrase = split[0]\n",
    "    split = phrase.split(' for ')\n",
    "    phrase = split[0]\n",
    "    \n",
    "    # move 'with x', 'in x' phrases to front\n",
    "    split = phrase.split(' with ')\n",
    "    if len(split) > 1:\n",
    "        phrase = ' '.join([split[1], split[0]])\n",
    "    split = phrase.split(' in ')\n",
    "    if len(split) > 1:\n",
    "        phrase = ' '.join([split[1], split[0]])\n",
    "    \n",
    "    # hacky spelling correction\n",
    "    for k, v in spellcheck_compiled:\n",
    "        phrase = k.sub(v, phrase) \n",
    "    \n",
    "    # substitute phrases\n",
    "    split = phrase.split()\n",
    "    for i, word in enumerate(split):\n",
    "        if word in words_to_sub: \n",
    "            split[i] = words_to_sub[word]\n",
    "    phrase = ' '.join(split)\n",
    "    for k, v in phrases_to_sub.items():\n",
    "        sub = ' ' + v + ' '\n",
    "        phrase = sub.join(phrase.split(k))\n",
    "    \n",
    "    # sub ' of (the)? ' with '-of-'\n",
    "    phrase = of_pattern.sub('-of-', phrase)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_cleaned = data.ingredients.map(lambda l: list(map(clean_phrase, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(phrases):\n",
    "    res = set()\n",
    "    for phrase in phrases:\n",
    "        if not phrase:\n",
    "            continue\n",
    "        split = phrase.split()\n",
    "        for i, word in enumerate(split):\n",
    "            if len(word.split('-')) > 1:\n",
    "                res.add(word)\n",
    "                split[i] = 'below' # stopword\n",
    "                continue\n",
    "            if len(word) > 4:\n",
    "                split[i] = correct_spelling(word)\n",
    "            if word in words_to_segment:\n",
    "                    split[i] = segment_word(word)\n",
    "        split = remove_dupes(split)\n",
    "        split = lemmatize(' '.join(split))\n",
    "        if not split:\n",
    "            continue\n",
    "        for word in split:\n",
    "            if word in lang_trans:\n",
    "                res.add('{}-l'.format(lang_trans[word]))\n",
    "        #if len(split) > 1:\n",
    "        #    res.add(' '.join(split[-2:]))\n",
    "        res.update(split)\n",
    "    return list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_cleaned = phrases_cleaned.map(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "_ = words_cleaned.map(words.extend)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = set()\n",
    "for word, count in word_counts.items():\n",
    "    if count < 3:\n",
    "        rare.add(word)\n",
    "len(rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in rare:\n",
    "    del word_counts[word]\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonrare = words_cleaned.map(lambda lst: [word for word in lst if not word in rare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ingredients = nonrare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('data/data_cleaned.csv', header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisines = data.cuisine.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_df = data[['cuisine', 'ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = defaultdict(list)\n",
    "for rowid, vals in ing_df.iterrows():\n",
    "    docs_list[vals.cuisine].extend(vals.ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_list['southern_us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = Counter(docs_list['italian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = Counter(docs_list['french'] + docs_list['southern_us'] + docs_list['russian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fk, fv in fr.items():\n",
    "    if fk in it:\n",
    "        iv = it[fk]\n",
    "        new_iv = iv - (iv**2 // (iv + fv))\n",
    "        if new_iv < 0:\n",
    "            new_iv = 0\n",
    "        it[fk] = new_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_it = []\n",
    "for k, v in it.items():\n",
    "    new_it.extend([k] * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list['italian'] = new_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for k, v in docs_list.items():\n",
    "    if k == 'test':\n",
    "        continue\n",
    "    docs[k] = ' '.join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_keys = []\n",
    "doc_vals = []\n",
    "for k, v in docs.items():\n",
    "    doc_keys.append(k)\n",
    "    doc_vals.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, \n",
    "                             strip_accents=None, token_pattern=r'[\\w-]+', analyzer='word', stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs = vectorizer.fit_transform(doc_vals)\n",
    "names = vectorizer.get_feature_names()\n",
    "tfidfs = pd.SparseDataFrame(tfidfs)\n",
    "tfidfs.index = doc_keys\n",
    "tfidfs.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make output for tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tfidfs.describe(percentiles=[.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_weights = summary.loc['std'] / summary.loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sums = tfidfs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pct = tfidfs / tfidf_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_get_scores(recipe):\n",
    "    weighted_tfidfs = tfidf_weights[recipe] * tfidfs[recipe]\n",
    "    return weighted_tfidfs.sum(axis=1) / len(recipe)\n",
    "\n",
    "def get_scores(recipe):\n",
    "    return tfidf_pct[recipe].sum(axis=1) / len(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Errors: x is misclassified as y\n",
    "brazilian as mexican, italian\n",
    "british as southern_us, french\n",
    "filipino as chinese\n",
    "french as italian, southern_us\n",
    "greek as italian\n",
    "irish as southern_us\n",
    "russian as italian, french\n",
    "spanish as italian\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs_recipes = data.ingredients.apply(get_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs_recipes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data, tfidfs_recipes), axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make output for ingredient features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {k: i for i, k in enumerate(word_counts)}\n",
    "col_names = [k for k, _ in sorted(indices.items(), key=itemgetter(1))]\n",
    "zeros = np.zeros((data.shape[0], len(col_names)), dtype=np.uint8)\n",
    "for ri, ings in enumerate(data.ingredients):\n",
    "    zeros[ri, [indices[ing] for ing in ings]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_df = pd.DataFrame(zeros, columns=col_names, index=data.index)\n",
    "zeros_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data, zeros_df), axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.query('cuisine != \"test\"')\n",
    "train = train.drop(columns=['ingredients'])\n",
    "train_cuisine = train.iloc[:,0]\n",
    "train_cuisine.to_csv('data/cuisine.csv', header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['cuisine'])\n",
    "train.to_csv('data/tfidf_it_red_train.csv', header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[data.iloc[:,0] != 'test']\n",
    "test = test.drop(columns=['cuisine', 'ingredients'])\n",
    "test.to_csv('data/tfidf_it_red_test.csv', header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

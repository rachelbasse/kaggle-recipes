{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# standard\n",
    "from csv import DictReader\n",
    "from operator import itemgetter\n",
    "import re\n",
    "\n",
    "# extra\n",
    "from funcy import memoize\n",
    "import spacy\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack instead of more complicated solution\n",
    "# WARNING: words of length <= 4 will not get spelling correction\n",
    "spellcheck = {\n",
    "    'reduc': 'reduced',\n",
    "    'jell o': 'gelatin',\n",
    "    'jello': 'gelatin',\n",
    "    'made with': 'with',\n",
    "    'v': 'tomato juice',\n",
    "    'e fu': 'yi mein',\n",
    "    'miracle whip': 'mayonaise',\n",
    "    'veget': 'vegetable',\n",
    "    'vegeta': 'vegetable',\n",
    "    'recip': 'recipe',\n",
    "    'oliv': 'olive',\n",
    "    'semi sweet': 'semisweet',\n",
    "    'mellow yellow': 'cola',\n",
    "    'mountain dew': 'cola',\n",
    "    'coke': 'cola',\n",
    "    'soi': 'soy',\n",
    "    'sauc': 'sauce',\n",
    "    'ground pepper': 'bpepper',\n",
    "    'black pepper': 'bpepper',\n",
    "    'of beef': 'beef',\n",
    "    'of veal': 'veal',\n",
    "    'of round': 'beef',\n",
    "    'abalone': 'snail',\n",
    "    'rosemari': 'rosemary',\n",
    "    'old bay seasoning': 'obseasoning',\n",
    "    'half and half': 'andhalfhalf'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['s', 't', 'n', 'l', 'lb',\n",
    "         'accompaniment', 'all', 'boil', 'bought', 'boston', 'boned', 'boneless', 'breast', 'breasts', 'broiler',\n",
    "         'can', 'canned', 'chopped', 'cracked', 'crumbled', 'crumbles', 'crushed', 'cubed', 'cuisine', 'cuisines', \n",
    "         'cut', 'center', 'coarse', 'cook', 'cooking', 'dried', 'dry', 'diced', 'extra', 'eye',\n",
    "         'fast', 'filled', 'fine', 'finely', 'flat', 'fresh', 'freshly', 'frozen', 'fryer', 'full', 'fully', \n",
    "         'granulate', 'granulated', 'grate', 'grated', 'great', 'grill', 'ground', \n",
    "         'half', 'halves', 'head', 'high', 'homestyl', 'homestyle', 'kernel', 'kitchen', \n",
    "         'large', 'leaf', 'leave', 'leftover', 'leg', 'legs', 'low',\n",
    "         'master', 'medium', 'mild', 'minced', 'mini', 'minicub', 'natural',\n",
    "         'original', 'ounc', 'oven', 'oz', 'part', 'plain', 'pod', 'premium', 'purpose', \n",
    "         'rack', 'ready', 'real', 'reduce', 'refrigerated', 'rising', 'round', 'seasoning', 'skinned', 'skinless', 'sodium',\n",
    "         'seamless', 'sheet', 'shred', 'shredded', 'slice', 'sliced', 'small', 'standing', 'streaky', 'store', 'style', 'superior', \n",
    "         'thigh', 'thighs', 'torn', 'traditional', 'unsweetened', 'wedges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sub_classes = {\n",
    "    '': stopwords,\n",
    "    'alcohol': ['bénédictine', 'budweiser', 'cordial', 'jagermeister', 'kirschenliqueur', 'kirschwasser', 'lambic', \n",
    "                'mixer', 'moonshine'],\n",
    "    'bird': ['pheasant'],\n",
    "    'fish': ['amberjack', 'bream', 'brill', 'carp', 'hamachi', 'lingcod', 'perch', 'pike', 'scrod', 'sockeye'],\n",
    "    'game': ['bear', 'bison', 'moose', 'rooster'],\n",
    "    'meat': ['bratwurst', 'chateaubriand', 'chopmeat', 'frankfurter', 'frog', 'jambon', 'knuckle', 'ribeye', 'riblet', 'wiener'],\n",
    "    'sweetener': ['sucralose', 'stevia', 'swerve', 'truvía'],\n",
    "    'tea': ['ceylon', 'pekoe', 'souchong'],\n",
    "    'american': ['argo', 'bisquick', 'bragg', 'braggs', 'breakstone', 'breakstones', 'breyers', 'crisco',\n",
    "              'campbells', 'diamond', 'frenchs', 'heinz', 'hellmanns', 'hellmann', 'hershey',\n",
    "              'hurst', 'jif', 'jiffy', 'johnsonville', 'jonshonville', 'kerrygold', 'klondike', 'knorr', 'knudsen', 'kraft', 'lipton',\n",
    "              'mazola', 'meyer', 'mccormick', 'nestle', 'pam', 'pace', 'philadelphia', 'pillsbury', 'pompeian', 'progresso',\n",
    "              'ragu', 'ritz', 'smithfield', 'sargento', 'stonefire', 'swanson', 'triscuits', 'tyson',\n",
    "              'wishbone', 'wondra', 'williams', 'yoplait', 'zatarains'],\n",
    "    'italian': ['barilla', 'bertolli', 'classico', 'delallo'],\n",
    "    'mexican': ['mission', 'goya', 'rotel', 'tostidos'],\n",
    "    'healthy': ['diet', 'fatfree', 'glutenfree', 'grassfed', 'light', 'lean', 'lowfat',  'lowsodium', 'nonfat']\n",
    "}\n",
    "\n",
    "# WARNING: these will not be surrounded by word boundaries\n",
    "phrase_sub_classes = {\n",
    "    '': ['firmly packed', 'flat leaf', 'free range', 'on the vine', 'rapid rise', 'vine ripened', 'whole kernel'],\n",
    "    'american': ['artisan blends', 'best food', 'better than bouillon', 'betty crocker', 'bob evans', 'calcium plus vitamin d', 'country crock',\n",
    "              'crystal farms', 'duncan hines', 'earth balance', 'egglands best', 'family harvest', 'farmhouse original', \n",
    "              'foster farms', 'franks redhot', 'gold medal', 'good seasons', 'gourmet garden',\n",
    "              'green giant', 'hidden valley', 'hillshire farms', 'home originals', 'honeysuckle white',\n",
    "              'i cant believe its not', 'i cant believe it not', 'i cant believ it not', 'jimmy dean', 'king arthur', 'land o lakes',\n",
    "              'lea and perrins', 'mrs dash', 'nielsen massey', 'no stick', 'oscar mayer', \n",
    "              'pasta sides', 'pepperidge farm', 'pure wesson', 'ready rice', 'recipe creation', 'recipe secret', 'robert mondavi',\n",
    "              'simply organic', 'skippy', 'special k', 'spice islands', 'a hint of', 'a touch of philadelphia', \n",
    "              'texas pete', 'uncle bens', 'wish bone', 'honey bunches of oats'],\n",
    "    'asian': ['a taste of thai', 'conimex woksaus specials', 'soy vay', 'veri veri'],\n",
    "    'italian': ['old world style'],\n",
    "    'mexican': ['old el paso', 'ro tel', 'taco bell', 'thick and chunky'],\n",
    "    'healthy': ['cholesterol free', 'fat free', 'gluten free', 'less sodium', 'low fat', 'low salt', 'low sodium', 'lower sodium', \n",
    "                'non fat', 'no salt added', 'reduced fat', 'reduce sodium', 'reduced sodium', 'sodium reduced', 'wheat free'],\n",
    "    # ands\n",
    "    'andmaccheese': ['macaroni and cheese'],\n",
    "    'andbreadbutter': ['bread and butter'],\n",
    "    'chocolates': ['m and ms'],\n",
    "    'andporkbeans': ['pork and beans'],\n",
    "    'andsweetsour': ['sweet and sour'],\n",
    "    # synonyms\n",
    "    'bpepper': ['black pepper', 'ground pepper'],\n",
    "    'garlic': ['garlic clove', 'garlic cloves'],\n",
    "    'sugar': ['white sugar', 'granulated sugar'],\n",
    "    'cream': ['whipping cream', 'heavy cream', 'heavy whipping cream'],\n",
    "    'cumin': ['cumin seed', 'cumin seeds']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_segment = {\n",
    "    'rockfish', 'cuttlefish', 'blackcurrant', 'pumpkinseed', 'breadcrumbs', 'crayfish', 'cauliflowerets', 'beetroot', 'chilegarlic', \n",
    "    'cornflour', 'almondmilk', 'crawfish', 'poundcake', 'wheatberries', 'kiwifruit', 'corncobs', 'applesauce', 'alfredostyle', \n",
    "    'chocolatecovered', 'sablefish', 'ducklings', 'sheepshead', 'sweetbreads', 'bellpepper', 'redcurrant', 'wolfberries', 'poppyseeds', \n",
    "    'crabapples', 'breadstick', 'milkfish', 'breadcrumb', 'blackpepper', 'cornbread', 'mexicorn', 'fruitcake', 'boysenberries', 'monkfish', \n",
    "    'huckleberries', 'swordfish', 'whitefish', 'shellfish', 'cornstarch', 'quickcooking', 'bluefish', 'kingfish'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert dict of {k1: v1, k2: v1, k3: v2} to {v1: [k1, k2], v2: k3}\n",
    "def invert_dict_singles(orig):\n",
    "    new = {}\n",
    "    for k, v in orig.items():\n",
    "        new[v] = new.get(v, [])\n",
    "        new[v].append(k)\n",
    "    return new\n",
    "\n",
    "# invert dict of {k1: v1, k2: v1, k3: v2} to {v1: [k1, k2], v2: k3}\n",
    "def invert_dict_lists(orig):\n",
    "    new = {}\n",
    "    for k, vals in orig.items():\n",
    "        for v in vals:\n",
    "            new[v] = k\n",
    "    return new\n",
    "\n",
    "# remove first dupes and keep order: [3, 1, 3, 2, 3] => [1, 2, 3]\n",
    "def remove_first_dupes(lst):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    res = [x for x in reversed(lst) if not (x in seen or seen_add(x))]\n",
    "    res = res[::-1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_pattern = re.compile(r'[®™’â€/\\!\\'%\\(\\)\\.\\d]')\n",
    "of_pattern = re.compile(r'(\\w) of (?:the )?')\n",
    "words_to_sub = invert_dict_lists(word_sub_classes)\n",
    "phrases_to_sub = invert_dict_lists(phrase_sub_classes)\n",
    "\n",
    "spellcheck_compiled = []\n",
    "for k, v in sorted(spellcheck.items()):\n",
    "    k = re.compile(r'(\\b)' + k + r'(\\b)')\n",
    "    v = r'\\1' + v + r'\\2'\n",
    "    spellcheck_compiled.append((k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected language strings to add as ingredients\n",
    "lang_trans = {}\n",
    "with open('data/lang_tags.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = DictReader(file, fieldnames=['word', 'lang'])\n",
    "    for row in reader:\n",
    "        lang_trans[row['word']] = row['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_spell = SymSpell(83000, 1, 7)\n",
    "dictionary_path = 'data/frequency_dictionary.txt'\n",
    "if not sym_spell.load_dictionary(dictionary_path, 0, 1):\n",
    "    print(\"Dictionary file not found\")\n",
    "\n",
    "def correct_spelling(word):\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.TOP, 1)\n",
    "    if not suggestions:\n",
    "        return word\n",
    "    return suggestions[0].term\n",
    "\n",
    "sym_seg = SymSpell(83000, 1, 8)\n",
    "dictionary_path = 'data/segment_dictionary.txt'\n",
    "if not sym_seg.load_dictionary(dictionary_path, 0, 1):\n",
    "    print(\"Dictionary file not found\")\n",
    "\n",
    "def segment_word(word):\n",
    "    res = sym_seg.word_segmentation(word)\n",
    "    return res.segmented_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "\n",
    "# workaround for 'en_core_web_lg' stop_words bug\n",
    "for word in parse.Defaults.stop_words:\n",
    "    lex = parse.vocab[word]\n",
    "    lex.is_stop = True\n",
    "\n",
    "@memoize\n",
    "def lemmatize(phrase):\n",
    "    is_stopword = lambda token: (token.is_stop or token.lemma_ in stopwords)\n",
    "    return [token.lemma_ for token in parse(phrase) if not is_stopword(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: don't overwrite the existing manually tweaked dictionary!!!\n",
    "def make_freq_dict(word_counts):\n",
    "    lines = []\n",
    "    for word, freq in sorted(word_counts.items(), key=itemgetter(1), reverse=True):\n",
    "        if freq > 4 and len(word) > 3:\n",
    "            lines.append('{0} {1}\\n'.format(word, freq))\n",
    "    with open('data/new_frequency_dictionary.txt', 'w+', encoding='utf-8') as file:\n",
    "        file.write(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
